{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAeiNleFRYt+GBtpzSDceH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/injoon-pij/pytorch-learning/blob/master/pytorch_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLsLph1ubMvf"
      },
      "source": [
        "# 1) RNN (character level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWkvA4hjcj-H"
      },
      "source": [
        "## (1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V44vjVfhTGfe"
      },
      "source": [
        "문자 시퀀스 apple을 입력받으면 pple!를 출력하는 RNN을 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRvGgNd7T5jN"
      },
      "source": [
        "### 1.1 Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVXk2t1ObFEH"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2H-sxJ-TMLw",
        "outputId": "6a64740d-508e-4dd3-9b66-9ac8fde114c3"
      },
      "source": [
        "# 입력 데이터와 레이블 데이터에 대한 문자 집합(voabulary)\n",
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "\n",
        "print ('문자 집합 : {}'.format(char_vocab))\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 : ['!', 'a', 'e', 'l', 'p']\n",
            "문자 집합의 크기 : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe_dnhVGTkWW"
      },
      "source": [
        "input_size = vocab_size # 입력의 크기는 문자 집합의 크기 (원-핫 벡터 사용)\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyeDa4edTujc",
        "outputId": "aac90c23-3b58-4b8e-bee0-b5d4cabcb988"
      },
      "source": [
        "# 문자에 고유한 정수 인덱스 부여\n",
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab))\n",
        "print(char_to_index)\n",
        "\n",
        "# 인덱스-문자 dict\n",
        "index_to_char = {}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n",
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N00voXs1UX4d",
        "outputId": "c9bd5dd5-25f3-4444-888c-c20c7b40f266"
      },
      "source": [
        "# 입력 데이터와 레이블 데이터 정수로 맵핑\n",
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB9pvBbGUmje",
        "outputId": "1877c9eb-7ac4-4006-ac65-ad2379d22285"
      },
      "source": [
        "# nn.RNN()은 기본적으로 3차원 텐서를 입력받기 때문에 배치 차원을 추가\n",
        "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAAwfs52WvEF",
        "outputId": "022f027e-bb82-4f3d-a3df-1828f8b32dbe"
      },
      "source": [
        "# 입력 데이터를 원핫 벡터로 변환\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z9oPBBRXqLO",
        "outputId": "bfc1a0f3-afff-4785-abc6-7fcd092f1489"
      },
      "source": [
        "# 입력 데이터와 레이블 데이터를 텐서로 변환\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZINy8cHX1bl"
      },
      "source": [
        "### 1.2 Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlb-iR-cX4aU"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
        "        self.fc = nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
        "\n",
        "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x) # x.shape = (batch_size, sequence_length, hidden_size)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvgGRV2HZOpV",
        "outputId": "47891fcb-016e-45e1-a5e1-89b57ac25c84"
      },
      "source": [
        "net = Net(input_size, hidden_size, output_size)\n",
        "\n",
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서 출력 (batch_size, sequence_length, output_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18X3qu9ngQNu"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnCydzEDab4v",
        "outputId": "e22166f5-58da-4747-c487-905946f88004"
      },
      "source": [
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
        "\n",
        "# train\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, output_size), Y.view(-1)) # view : Batch 차원 제거\n",
        "    loss.backward()\n",
        "    optimizer.step() \n",
        "\n",
        "    # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "    result = outputs.data.numpy().argmax(axis=2) # outputs.data.numpy() : tensor를 numpy로 변환\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss:  1.5707448720932007 prediction:  [[4 2 2 2 3]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  peeel\n",
            "1 loss:  1.2935644388198853 prediction:  [[4 4 2 2 2]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppeee\n",
            "2 loss:  1.0445367097854614 prediction:  [[4 4 3 2 2]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplee\n",
            "3 loss:  0.8069427609443665 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "4 loss:  0.6217581629753113 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "5 loss:  0.4866600036621094 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "6 loss:  0.3835570812225342 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "7 loss:  0.29876938462257385 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.22924959659576416 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.17240861058235168 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.12800326943397522 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.09549745917320251 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.071721151471138 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.05693908408284187 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.04442741349339485 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.03570036217570305 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.02866079844534397 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.022974757477641106 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.018639257177710533 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.0154116777703166 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.012998935766518116 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.01116853766143322 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.009758208878338337 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.008655304089188576 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.00777845224365592 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.0070680589415133 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.006480080075562 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.0059831710532307625 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.005554923787713051 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0051797786727547646 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.004846855998039246 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.004548503551632166 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.00427936390042305 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.004035391844809055 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.0038136013317853212 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.0036114011891186237 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0034268845338374376 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.003258212935179472 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.003103854600340128 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.002962297759950161 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.002832315396517515 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0027127270586788654 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.0026025634724646807 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.0025008562952280045 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0024068490602076054 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0023198327980935574 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0022392633836716413 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.002164479112252593 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0020951018668711185 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.0020306813530623913 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0019708876498043537 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0019152937456965446 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.001863592304289341 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0018154991557821631 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0017706584185361862 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0017287622904404998 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.001689644530415535 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0016531400615349412 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0016188208246603608 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0015866633038967848 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.00155638309661299 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0015278136124834418 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0015008365735411644 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.0014752853894606233 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0014510175678879023 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.001428009825758636 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0014060480752959847 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0013851083349436522 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.001365072326734662 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.001345868338830769 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.001327425241470337 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.0013097668997943401 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.0012926316121593118 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.0012762099504470825 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.0012603352079167962 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0012449122732505202 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.001230036374181509 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.00121554103679955 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.0012015452375635505 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.0011878346558660269 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.0011745525989681482 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.0011616271222010255 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.0011489870958030224 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0011366326361894608 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.0011245872592553496 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0011127323377877474 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.001101210480555892 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.0010899024782702327 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0010788326617330313 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0010679292026907206 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.001057335059158504 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.00104683602694422 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0010365511989220977 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.0010264329612255096 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.0010164810810238123 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.0010067196562886238 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0009971248218789697 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.0009876249823719263 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.0009783155983313918 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.0009691250743344426 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNH8lO8jcmaG"
      },
      "source": [
        "## (2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pRbrCxghT8P"
      },
      "source": [
        "더 많은 데이터에 대해 문자 단위 RNN을 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx5qJ66QikRi"
      },
      "source": [
        "### 2.1 Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANUfx1XpcnLP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3431El-hZJ0"
      },
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1pX_tBNhcGo",
        "outputId": "58e61272-3320-47b1-a512-a044dfe5734b"
      },
      "source": [
        "char_set = list(set(sentence)) # vocabulary\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))\n",
        "print(char_dic)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 25\n",
            "{'b': 0, 'g': 1, 'p': 2, 'l': 3, 'i': 4, 't': 5, 'u': 6, 'h': 7, 'o': 8, '.': 9, 'k': 10, 'w': 11, 'm': 12, 's': 13, 'd': 14, 'y': 15, 'c': 16, ' ': 17, 'n': 18, 'a': 19, ',': 20, 'f': 21, 'e': 22, 'r': 23, \"'\": 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_POwFuMXh6Au"
      },
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10  # 임의 숫자 지정\n",
        "learning_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMWrPujvhkP5",
        "outputId": "d0177bc7-5bcc-4f43-dc32-42b8b489afec"
      },
      "source": [
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i : i+sequence_length]\n",
        "    y_str = sentence[i+1: i+sequence_length+1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvGhNmoAiXmx",
        "outputId": "4aa0fac6-98e0-452e-8134-67f2125fcbfd"
      },
      "source": [
        "print(x_data[0]) # if you wan\n",
        "print(y_data[0]) # f you want"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 21, 17, 15, 8, 6, 17, 11, 19, 18]\n",
            "[21, 17, 15, 8, 6, 17, 11, 19, 18, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtmqD1gmjDFq",
        "outputId": "ef4ba3c2-c3d4-4e92-b66d-2c373b09fa1c"
      },
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kJVJNdJjxBf"
      },
      "source": [
        "### 2.2 Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1RoXFcijye-"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9Abg2oaj60H"
      },
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjytPSnfk27Y",
        "outputId": "9fb22216-e3bf-4b08-deb9-d448833924ae"
      },
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84b94_9Nk_hn",
        "outputId": "94fbb053-802b-49af-ab3f-9b7b82e6f24e"
      },
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) # input_data size (170, 10, 25) \n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    if i % 10 == 0:\n",
        "      print(\"epoch {}\".format(i), predict_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 ggeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeageeeeeeeeegeeeeeeeeeegeeeeeeeeeeegeeeeeeeeeeeegeeeeeeeeeeeeeeeeeeegeeeeeeeeeeegeeeeeeeeeeeegeeeeeeeeeeeeeeegeeeeeeeeeegeeeeegee\n",
            "epoch 10   dp i a d d  t d e s i tas d e t t dt d  t    t t e   e  d d s   it w    t s w e t t et t w e  w ect t d d n s d d d sa m d d   a  m w e'  t a nps d  aate   t      t em  t ea a  \n",
            "epoch 20 gyteo tand to puile tod ips ton't doum ap ponp e theothem thrpo ees, wo p tnd ton't dssigo them to cs tnd torkt tut duthem toncs them tonpon' poe themsnd ess tu ens ty am themsens\n",
            "epoch 30 gyy uhwan' to build aothip, don't urum up people th ether th collect word wnd won't ussign them tosks and work, dut rather thach them to bong for themsnd e s tmmensity on themseas\n",
            "epoch 40 thy u want to build a ship, don't urum up people together to collect wood and won't ussign them tosks and work, dut rather toach them to long for themendless immensity of themeea.\n",
            "epoch 50 t you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for themendless immensity of themeea.\n",
            "epoch 60 p you want to build a ship, don't drum up people together te collect wood and don't dssign the  tosks and work, but rather teach the  to long for the endless immensity of the eea.\n",
            "epoch 70 t you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the eea.\n",
            "epoch 80 t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "epoch 90 t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA5XosYXbdwt"
      },
      "source": [
        "# 2) RNN (word level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQuVby36psEO"
      },
      "source": [
        "__'Repeat is the best medicine for'__을 입력받으면 __'is the best medicine for memory'__를 출력하는 RNN 모델 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n76qAKEupeyQ"
      },
      "source": [
        "## 2.1 Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjuIVITUb5eF"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lnoTbPlplEl",
        "outputId": "b8c0c3f0-e56e-4a0e-bc09-106452fc0a0f"
      },
      "source": [
        "sentence = \"Repeat is the best medicine for memory\".split()\n",
        "\n",
        "# vocab\n",
        "vocab = list(set(sentence))\n",
        "print(vocab)\n",
        "\n",
        "# 단어에 고유한 정수 부여\n",
        "word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}\n",
        "word2index['<unk>']=0\n",
        "print(word2index)\n",
        "\n",
        "# idx2word dictionary \n",
        "index2word = {v: k for k, v in word2index.items()}\n",
        "print(index2word)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['for', 'memory', 'the', 'medicine', 'best', 'Repeat', 'is']\n",
            "{'for': 1, 'memory': 2, 'the': 3, 'medicine': 4, 'best': 5, 'Repeat': 6, 'is': 7, '<unk>': 0}\n",
            "{1: 'for', 2: 'memory', 3: 'the', 4: 'medicine', 5: 'best', 6: 'Repeat', 7: 'is', 0: '<unk>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWf6FsIzrnyr"
      },
      "source": [
        "def build_data(sentence, word2index):\n",
        "    encoded = [word2index[token] for token in sentence] # 각 문자를 정수로 변환. \n",
        "    input_seq, label_seq = encoded[:-1], encoded[1:] # 입력 시퀀스와 레이블 시퀀스를 분리\n",
        "    input_seq = torch.LongTensor(input_seq).unsqueeze(0) # 배치 차원 추가\n",
        "    label_seq = torch.LongTensor(label_seq).unsqueeze(0) # 배치 차원 추가\n",
        "    return input_seq, label_seq"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD1jyLi3rooT",
        "outputId": "ff0542af-a7d3-4ed7-8aaa-e65060340cb3"
      },
      "source": [
        "X, Y = build_data(sentence, word2index)\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6, 7, 3, 5, 4, 1]])\n",
            "tensor([[7, 3, 5, 4, 1, 2]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOTg8lEXsbde"
      },
      "source": [
        "## 2.2 Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vru86q2Psj1K"
      },
      "source": [
        "이전 모델들과 달라진 점은 임베딩 층을 추가했다는 것. (nn.Embedding())\n",
        "\n",
        "임베딩층은 크게 두 가지 인자를 받는데 첫번째 인자는 단어장의 크기이며, 두번째 인자는 임베딩 벡터의 차원임"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQnaoFrFsdgv"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings = vocab_size, embedding_dim = input_size)\n",
        "        self.rnn_layer = nn.RNN(input_size, hidden_size, batch_first = batch_first)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size) # 출력 크기는 단어 집합의 크기와 같아야함\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        output = self.embedding_layer(x)\n",
        "        # output : (배치 크기, 시퀀스 길이, 은닉층 크기), hidden : (1, 배치 크기, 은닉층 크기)\n",
        "        output, hidden = self.rnn_layer(output)\n",
        "        # (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n",
        "        output = self.linear(output)\n",
        "        # (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n",
        "        return output.view(-1, output.size(2))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RRdQgPgs1MH"
      },
      "source": [
        "# hyperparameter\n",
        "vocab_size = len(word2index)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다. <unk> 토큰을 크기에 포함한다.\n",
        "input_size = 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
        "hidden_size = 20  # RNN의 은닉층 크기\n",
        "\n",
        "# model\n",
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
        "\n",
        "# cost function\n",
        "loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 레이블값은 원-핫 인코딩 안 해도 됨.\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(params=model.parameters())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcy8v9yjvRKP",
        "outputId": "bec263c3-88df-4bcb-b73c-dd70e52db21a"
      },
      "source": [
        "output = model(X)\n",
        "print(output)\n",
        "print(output.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1243,  0.0961, -0.0696, -0.1287,  0.3780,  0.0083,  0.0472,  0.0020],\n",
            "        [ 0.1929,  0.2069, -0.3637, -0.2539,  0.1969,  0.3460,  0.0099, -0.2718],\n",
            "        [ 0.2203,  0.3917, -0.3413, -0.1028,  0.1934,  0.0407, -0.2451,  0.0736],\n",
            "        [ 0.2316,  0.1667,  0.0456, -0.1516,  0.2706,  0.1618,  0.1669, -0.1120],\n",
            "        [ 0.1085,  0.3258, -0.3764, -0.0701,  0.1445,  0.2894, -0.5274,  0.1147],\n",
            "        [ 0.3694,  0.0239, -0.3853,  0.0057,  0.1107,  0.5964, -0.3665, -0.0027]],\n",
            "       grad_fn=<ViewBackward>)\n",
            "torch.Size([6, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1imhkjqCvX7W"
      },
      "source": [
        "# 수치화된 데이터를 단어로 전환하는 함수\n",
        "decode = lambda y: [index2word.get(x) for x in y]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxND_fq-vg1P",
        "outputId": "363f097a-abdf-49b3-a267-fc2d859fe748"
      },
      "source": [
        "# 훈련 시작\n",
        "for step in range(201):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X)\n",
        "\n",
        "    loss = loss_function(output, Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if step % 40 == 0:\n",
        "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
        "        pred = output.softmax(-1).argmax(-1).tolist()\n",
        "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
        "        print()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/201] 2.1465 \n",
            "Repeat medicine best for medicine for best\n",
            "\n",
            "[41/201] 1.5476 \n",
            "Repeat medicine for best medicine for memory\n",
            "\n",
            "[81/201] 0.9017 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[121/201] 0.4554 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[161/201] 0.2289 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[201/201] 0.1312 \n",
            "Repeat is the best medicine for memory\n",
            "\n"
          ]
        }
      ]
    }
  ]
}